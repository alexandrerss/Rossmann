{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.0 IMPORTAR BIBLIOTECAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import inflection\n",
    "import json\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import random\n",
    "import requests\n",
    "import seaborn as sns\n",
    "import sweetviz as sv\n",
    "import warnings\n",
    "import xgboost as xgb\n",
    "\n",
    "from boruta import BorutaPy\n",
    "from IPython.core.display import HTML\n",
    "from IPython.display import Image\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy import stats as ss\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression, Lasso\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.preprocessing import RobustScaler, MinMaxScaler, LabelEncoder\n",
    "from tabulate import tabulate\n",
    "\n",
    "warnings.filterwarnings( 'ignore' )\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajustar gráficos\n",
    "plt.rcParams['figure.figsize'] = [20, 10]\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.1 Funções"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cramer_v( x, y ):\n",
    "  cm = pd.crosstab( x, y ).values\n",
    "  n = cm.sum()\n",
    "  r, k = cm.shape\n",
    "\n",
    "  chi2 = ss.chi2_contingency( cm )[0]\n",
    "  chi2corr = max( 0, chi2 - (k-1)*(r-1)/(n-1) )\n",
    "  \n",
    "  kcorr = k - (k-1)**2/(n-1)\n",
    "  rcorr = r - (r-1)**2/(n-1)\n",
    "  return np.sqrt( (chi2corr/n) / ( min( kcorr-1, rcorr-1 ) ) )\n",
    "\n",
    "def mean_percentage_error( y, yhat ):\n",
    "  return np.mean( ( y - yhat ) / y )\n",
    "\n",
    "def mean_absolute_percentage_error( y, yhat ):\n",
    "  return np.mean( np.abs( ( y - yhat ) / y ) )\n",
    "\n",
    "def ml_error( model_name, y, yhat ):\n",
    "    mae = mean_absolute_error( y, yhat )\n",
    "    mape = mean_absolute_percentage_error( y, yhat )\n",
    "    rmse = np.sqrt( mean_squared_error( y, yhat ) )\n",
    "    return pd.DataFrame( { 'Model Name': model_name,\n",
    "    'MAE': mae,\n",
    "    'MAPE': mape,\n",
    "    'RMSE': rmse }, index=[0] )\n",
    "\n",
    "def cross_validation( x_training, kfold,model_name,model):\n",
    "    mae_list = []\n",
    "    mape_list = []\n",
    "    rmse_list = []\n",
    "    for fold in reversed( range( 1,kfold+1) ):\n",
    "        print('\\nKFold Number: {}'.format( fold ) )\n",
    "        # start and end date for validation\n",
    "        validation_start_date = x_training['date'].max() - datetime.timedelta(days=fold*6*7)\n",
    "        validation_end_date = x_training['date'].max() - datetime.timedelta(days=(fold-1)*6*7)\n",
    "\n",
    "        # filtering dataset\n",
    "        training = x_training[x_training['date'] < validation_start_date]\n",
    "        validation = x_training[(x_training['date'] >= validation_start_date) & (x_training['date'] <= validation_end_date)]\n",
    "\n",
    "        # training and validation dataset\n",
    "\n",
    "        # training\n",
    "        xtraining = training.drop( ['date', 'sales'], axis=1 )\n",
    "        ytraining = training['sales']\n",
    "\n",
    "        # validation\n",
    "        xvalidation = validation.drop( ['date', 'sales'], axis=1 )\n",
    "        yvalidation = validation['sales']\n",
    "\n",
    "        # model\n",
    "        m = model.fit( xtraining, ytraining )\n",
    "\n",
    "        # prediction\n",
    "        y_pred = m.predict( xvalidation )\n",
    "\n",
    "        # performance\n",
    "        m_result = ml_error( model_name, np.expm1( yvalidation ), np.expm1(y_pred ) )\n",
    "\n",
    "        # store performance of each kfold iteration\n",
    "        mae_list.append( m_result['MAE'] )\n",
    "        mape_list.append( m_result['MAPE'] )\n",
    "        rmse_list.append( m_result['RMSE'] )\n",
    "\n",
    "    return pd.DataFrame( {'Model Name': model_name,\n",
    "                          'MAE CV': np.round( np.mean( mae_list ), 2 ).astype(str ) + ' +/- ' + np.round( np.std( mae_list ), 2 ).astype( str ),\n",
    "                          'MAPE CV': np.round( np.mean( mape_list ), 2 ).astype( str ) + ' +/- ' + np.round( np.std( mape_list ), 2 ).astype( str ),\n",
    "                          'RMSE CV': np.round( np.mean( rmse_list ), 2 ).astype( str ) + ' +/- ' + np.round( np.std( rmse_list ), 2 ).astype( str )}, index=[0] )    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.2 Carregar base de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sales_raw = pd.read_csv( '../datasets/train.csv', low_memory=False )\n",
    "df_store_raw = pd.read_csv( '../datasets/store.csv', low_memory=False )\n",
    "\n",
    "# merge entre os dois dataframes\n",
    "df_raw = pd.merge( df_sales_raw, df_store_raw, how='left', on='Store' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.0 DESCRICAO DOS DADOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df_raw.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renomear as colunas para mdoelo snakecase\n",
    "\n",
    "cols_old = df1.columns\n",
    "snakecase = lambda x: inflection.underscore( x )\n",
    "cols_new = list( map( snakecase, cols_old ) )\n",
    "df1.columns = cols_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Tamanho dos dados (shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( 'Number of Rows: {}'.format( df1.shape[0] ) )\n",
    "print( 'Number of Cols: {}'.format( df1.shape[1] ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Tipo dos Dados (Dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tranaformar a coluna Date para formato de data\n",
    "df1['date'] = pd.to_datetime( df1['date'] )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4. Checagem de N/As"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.isna().sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como proceder para a retirada dos NAs ?\n",
    "\n",
    "- Descartar todas as linhas com NA.\n",
    "- Usando algoritmos de ML para inserir dados.\n",
    "- Entendendo o negócio para inserir dados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Preencher os NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# competition_distance              \n",
    "### Para todos os campos NA nesta coluna considerar e inserir o valor de 200.000 (valor máximo escolhido para este df)\n",
    "df1['competition_distance'] = df1['competition_distance'].apply(lambda x: 200000 if math.isnan(x) else x )\n",
    "\n",
    "# competition_open_since_month  \n",
    "### Para todos os campos NA nesta coluna considerar e inserir o valor do mês da venda (Coluna \"date\").\n",
    "df1['competition_open_since_month'] = df1.apply(lambda x: x['date'].month if math.isnan(x['competition_open_since_month']) else x['competition_open_since_month'],axis=1)\n",
    "\n",
    "# competition_open_since_year       \n",
    "### Para todos os campos NA nesta coluna considerar e inserir o valor do ano da venda (Coluna \"date\").\n",
    "df1['competition_open_since_year'] = df1.apply(lambda x: x['date'].year if math.isnan(x['competition_open_since_year']) else x['competition_open_since_year'],axis=1)\n",
    "\n",
    "# promo2_since_week \n",
    "### Para todos os campos NA nesta coluna considerar e inserir o valor da semana da venda (Coluna \"date\").\n",
    "df1['promo2_since_week'] = df1.apply(lambda x: x['date'].week if math.isnan(x['promo2_since_week']) else x['promo2_since_week'],axis=1)\n",
    "\n",
    "# promo2_since_year\n",
    "### Para todos os campos NA nesta coluna considerar e inserir o valor do ano da venda (Coluna \"date\").\n",
    "df1['promo2_since_year'] = df1.apply(lambda x: x['date'].year if math.isnan(x['promo2_since_year']) else x['promo2_since_year'],axis=1)\n",
    "\n",
    "# promo_interval\n",
    "month_map = {1: 'Jan',2: 'Fev',3: 'Mar',4: 'Apr',5: 'May',6: 'Jun',7: 'Jul',8: 'Aug', 9: 'Sep',10: 'Oct',11: 'Nov',12: 'Dec'}\n",
    "df1['promo_interval'].fillna(0, inplace=True) # substituir todos os NA para 0 (zero)\n",
    "df1['month_map'] = df1['date'].dt.month.map(month_map) # criar uma nova coluna e inserir o mês da coluna \"Date\" conforme a variavel \"month_map\"\n",
    "df1['is_promo'] = df1[['promo_interval','month_map']].apply( lambda x: 0 if x['promo_interval'] == 0 else 1 if x['month_map'] in x['promo_interval'].split(',') else 0, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.sample(5).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 Tipos de Mudança"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformar valores para inteiros\n",
    "df1['competition_open_since_month'] = df1['competition_open_since_month'].astype('int64')\n",
    "df1['competition_open_since_year'] = df1['competition_open_since_year'].astype('int64')\n",
    "df1['promo2_since_week'] = df1['promo2_since_week'].astype('int64')\n",
    "df1['promo2_since_year'] = df1['promo2_since_year'].astype('int64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.7 Descrição Estatística dos dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Métricas de tendência central : \n",
    "\n",
    "- media \n",
    "- mediana\n",
    "\n",
    "Métricas de dispersão : \n",
    "\n",
    "- desvio padrão\n",
    "- valor minimo\n",
    "- valor máximo\n",
    "- faixa(range) \n",
    "- skew(inclinação)\n",
    "- curtose "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numericos = df1.select_dtypes(include=['int64','float64' ])\n",
    "categoricos = df1.select_dtypes(exclude=['int64','float64','datetime64[ns]'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7.1. Atributos Numéricos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Métricas de tendência central \n",
    "media = pd.DataFrame ( numericos.apply(np.mean)).T\n",
    "mediana = pd.DataFrame ( numericos.apply(np.median)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Métricas de dispersão\n",
    "desvio_padrao = pd.DataFrame ( numericos.apply(np.std)).T\n",
    "minimo = pd.DataFrame ( numericos.apply(min)).T\n",
    "maximo = pd.DataFrame ( numericos.apply(max)).T\n",
    "faixa = pd.DataFrame ( numericos.apply(lambda x: x.max() - x.min())).T\n",
    "inclinação = pd.DataFrame ( numericos.apply(lambda x: x.skew() )).T\n",
    "curtose = pd.DataFrame ( numericos.apply(lambda x: x.kurtosis())).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concatenar\n",
    "metricas = pd.concat([minimo, maximo, faixa, media, mediana, desvio_padrao, inclinação, curtose ]).T.reset_index()\n",
    "metricas.columns = ['attributes','min','max','range','mean', 'median', 'std', 'skew','kurtosis' ]\n",
    "metricas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7.2. Atributos Categóricos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantos niveis cada variável categorica possui ?\n",
    "categoricos.apply( lambda x: x.unique().shape[0] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtro = df1[(df1['state_holiday'] != '0') & (df1['sales'] > 0)]\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "sns.boxplot(x='state_holiday', y='sales', data=filtro, hue='state_holiday', palette='deep', legend=False)\n",
    "\n",
    "plt.subplot( 1, 3, 2 )\n",
    "sns.boxplot( x='store_type', y='sales', data=filtro, hue='store_type', palette='deep', legend=False)\n",
    "\n",
    "plt.subplot( 1, 3, 3 )\n",
    "sns.boxplot( x='assortment', y='sales', data=filtro, hue='assortment', palette='deep', legend=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.0 FEATURE ENGINEERING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Mapa mental de hipóteses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image('../img/mindmap.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Criação das Hipoteses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 Hipoteses da loja\n",
    "\n",
    "1. Lojas com número maior de funcionários deveriam vender mais.\n",
    "2. Lojas com maior capacidade de estoque deveriam vender mais.\n",
    "3. Lojas com maior porte deveriam vender mais.\n",
    "4. Lojas com maior sortimentos deveriam vender mais.\n",
    "5. Lojas com competidores mais próximos deveriam vender menos.\n",
    "6. Lojas com competidores à mais tempo deveriam vendem mais."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 Hipoteses do produto\n",
    "\n",
    "1. Lojas que investem mais em Marketing deveriam vender mais.\n",
    "2. Lojas com maior exposição de produto deveriam vender mais.\n",
    "3. Lojas com produtos com preço menor deveriam vender mais.\n",
    "5. Lojas com promoções mais agressivas ( descontos maiores ), deveriam vender mais.\n",
    "6. Lojas com promoções ativas por mais tempo deveriam vender mais.\n",
    "7. Lojas com mais dias de promoção deveriam vender mais.\n",
    "8. Lojas com mais promoções consecutivas deveriam vender mais.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.3 Hipoteses do tempo\n",
    "\n",
    "1. Lojas abertas durante o feriado de Natal deveriam vender mais.\n",
    "2. Lojas deveriam vender mais ao longo dos anos.\n",
    "3. Lojas deveriam vender mais no segundo semestre do ano.\n",
    "4. Lojas deveriam vender mais depois do dia 10 de cada mês.\n",
    "5. Lojas deveriam vender menos aos finais de semana.\n",
    "6. Lojas deveriam vender menos durante os feriados escolares."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Lista Final das Hipoteses\n",
    "\n",
    "1. Lojas com maior sortimentos deveriam vender mais.\n",
    "2. Lojas com competidores mais próximos deveriam vender menos.\n",
    "3. Lojas com competidores à mais tempo deveriam vendem mais.\n",
    "4. Lojas com promoções ativas por mais tempo deveriam vender mais.\n",
    "5. Lojas com mais dias de promoção deveriam vender mais.\n",
    "6. Lojas com mais promoções consecutivas deveriam vender mais.\n",
    "7. Lojas abertas durante o feriado de Natal deveriam vender mais.\n",
    "7. Lojas deveriam vender mais ao longo dos anos.\n",
    "9. Lojas deveriam vender mais no segundo semestre do ano.\n",
    "10. Lojas deveriam vender mais depois do dia 10 de cada mês.\n",
    "12. Lojas deveriam vender menos aos finais de semana.\n",
    "12. Lojas deveriam vender menos durante os feriados escolares."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df1.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ano\n",
    "df2['year'] = df2['date'].dt.year\n",
    "\n",
    "#Mês\n",
    "df2['month'] = df2['date'].dt.month\n",
    "\n",
    "#Dia\n",
    "df2['day'] = df2['date'].dt.day\n",
    "\n",
    "#Dia da Semana\n",
    "df2['week_of_year'] = df2['date'].dt.isocalendar().week\n",
    "\n",
    "# Semana no Ano\n",
    "df2['year_week'] = df2['date'].dt.strftime('%Y-%W')\n",
    "\n",
    "# Data em dias de competição\n",
    "df2['competition_since'] = df2.apply(lambda x: datetime.datetime(year=x['competition_open_since_year'], month=x['competition_open_since_month'], day= 1), axis=1)\n",
    "df2['competition_time_month'] = ((df2['date'] - df2['competition_since'])/30).apply(lambda x: x.days).astype(int)\n",
    "\n",
    "# Data em dias de promoção\n",
    "df2['promo_since'] = df2['promo2_since_year'].astype(str) + '-' +df2['promo2_since_week'].astype(str)\n",
    "df2['promo_since'] = df2['promo_since'].apply( lambda x: datetime.datetime.strptime( x + '-1', '%Y-%W-%w' ) - datetime.timedelta( days=7 ) )\n",
    "df2['promo_time_week'] = ( ( df2['date'] - df2['promo_since'] )/7 ).apply(lambda x: x.days ).astype( int )\n",
    "\n",
    "# categorias\n",
    "df2['assortment'] = df2['assortment'].apply( lambda x: 'basic' if x == 'a' else 'extra' if x == 'b' else 'extended' )\n",
    "\n",
    "# feriados\n",
    "df2['state_holiday'] = df2['state_holiday'].apply( lambda x: 'public_holiday' if x == 'a' else 'easter_holiday' if x == 'b' else 'christmas' if x == 'c' else 'regular_day' )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.sample(5).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.0 Filtragem de Variáveis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df2.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Filtragem das Linhas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filltro para considerar a coluna Opem diferentes de zero (lojas abertas) e Sales maior que zero (Vendas)\n",
    "df3 = df3[(df3['open'] != 0) & (df3['sales'] > 0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Filtragem das Colunas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colunas_para_dropar = ['customers', 'open', 'promo_interval','month_map']\n",
    "df3 = df3.drop(colunas_para_dropar, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.0 EDA - Análise Exploratória de Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = df3.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Análise Univariada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A váriavel resposta para este projeto é a coluna \"Sales\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(df4['sales'], kde=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.2 Variável numérica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numericos.hist(bins=25);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Store : Numeração de cada loja, grafico pouco explicativo\n",
    "- Day of week : Valores de vendas parecidos em todos os dias\n",
    "- Sales : Valores de venda concentrado em até 20k\n",
    "- Customers : Valres da quantidade de clientes/dia concentrado até em 2k\n",
    "- Open : Maioria das lojas abertas, valor =1\n",
    "- Promo : Maioria das vendas em dias de não promoção, valor=0\n",
    "- SchoolHoliday : Maioria das vendas em dias de aulas normais(sem férias), valor=0\n",
    "- CompetitionDistance : Valres da quantidade de distencias concentrado até em 2k\n",
    "- competition_open_since_month : maiores vendas em Setembro, Abril e Novembro.Piores em Janeiro e Agosto.\n",
    "- competition_open_since_year : maiores vendas no último ano\n",
    "- promo2: valores muito parecidos, pouco relevante\n",
    "- promo2_since_week : comportamento do grafico se parece um pouco com o competition_open_since_month\n",
    "- promo2_since_year : aqui as maiores vendas sao concentradas em 2013\n",
    "- is_promo : maioria com valor = 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = sv.analyze(numericos)\n",
    "report.show_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.2 Variável categórica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categoricos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# state_holiday\n",
    "a = df4[df4['state_holiday'] != 'regular_day']\n",
    "sns.countplot(x=a['state_holiday'], data=a, hue='state_holiday', palette='deep', legend=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# state_holiday\n",
    "sns.kdeplot( df4[df4['state_holiday'] == 'public_holiday']['sales'],label='public_holiday', fill=True )\n",
    "sns.kdeplot( df4[df4['state_holiday'] == 'easter_holiday']['sales'],label='easter_holiday', fill=True )\n",
    "sns.kdeplot( df4[df4['state_holiday'] == 'christmas']['sales'],label='christmas', fill=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store_type\n",
    "sns.countplot(x=df4['store_type'],hue=df4['store_type'], palette='deep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store_type\n",
    "sns.kdeplot( df4[df4['store_type'] == 'a']['sales'],label='a', fill=True )\n",
    "sns.kdeplot( df4[df4['store_type'] == 'b']['sales'],label='b', fill=True )\n",
    "sns.kdeplot( df4[df4['store_type'] == 'c']['sales'],label='c', fill=True )\n",
    "sns.kdeplot( df4[df4['store_type'] == 'd']['sales'],label='d', fill=True )\n",
    "plt.legend(title='Store Type', loc='upper right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assortment\n",
    "sns.countplot(x=df4['assortment'], hue=df4['assortment'], palette='deep', legend=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assortment\n",
    "sns.kdeplot( df4[df4['assortment'] == 'basic']['sales'],label='basic', fill=True )\n",
    "sns.kdeplot( df4[df4['assortment'] == 'extended']['sales'],label='extended', fill=True )\n",
    "sns.kdeplot( df4[df4['assortment'] == 'extra']['sales'],label='extra', fill=True )\n",
    "plt.legend(title='Store Type', loc='upper right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Análise Bivariada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Lojas com maior sortimentos deveriam vender mais.\n",
    "\n",
    "FALSO = O \"Extra\" (maior sortimento) vendem menos.\n",
    "\n",
    "- De acordo com o primeiro gráfico, a loja EXTRA é a que menos vende.\n",
    "- De acordo com o segundo gráfico, as lojas BASIC e EXTENDED possuem variação de mensal mensal muito parecidos de uma com a outra.\n",
    "- De acordo com o segundo gráfico, a variação de vendas das lojas EXTRA nãoé linear como mostra os segundo gráfico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total de vendas por sortimento\n",
    "h1 = df4[['assortment','sales']].groupby('assortment').sum().reset_index()\n",
    "sns.barplot(h1, x='assortment', y='sales',hue='assortment', palette='deep', legend=False)\n",
    "\n",
    "\n",
    "# grafico em linha de vendas mensal\n",
    "aux2 = df4[['year_week', 'assortment', 'sales']].groupby(['year_week','assortment'] ).sum().reset_index()\n",
    "aux2.pivot( index='year_week', columns='assortment', values='sales' ).plot()\n",
    "\n",
    "# grafico em linha de vendas mensal apenas a linha \"extra\"\n",
    "aux3 = aux2[aux2['assortment'] == 'extra']\n",
    "aux3.pivot( index='year_week', columns='assortment', values='sales' ).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Lojas com competidores mais próximos deveriam vender menos.\n",
    "\n",
    "FALSO = Lojas mais próximas vendem mais\n",
    "\n",
    "- Maior quantidade de vendas são as lojas com distâncias de até 1 Km"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h2 =  df4[['competition_distance', 'sales']].groupby( 'competition_distance' ).sum().reset_index()\n",
    "sns.barplot(h2, x='competition_distance', y='sales')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#criando bins\n",
    "bins = list( np.arange( 0, 20000, 1000) )\n",
    "h2['competition_distance_binned'] = pd.cut( h2['competition_distance'],bins=bins )\n",
    "\n",
    "h21 = h2[['competition_distance_binned', 'sales']].groupby('competition_distance_binned' ).sum().reset_index()\n",
    "sns.barplot( x='competition_distance_binned', y='sales', data=h21 )\n",
    "plt.xticks( rotation=90 );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grafico de bolinhas\n",
    "h2 = df4[['competition_distance', 'sales']].groupby( 'competition_distance' ).sum().reset_index()\n",
    "sns.scatterplot( x ='competition_distance', y='sales', data=h2 );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Força da correlação\n",
    "sns.heatmap( h2.corr( method='pearson'), annot=True )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Lojas com competidores à mais tempo deveriam vendem mais.\n",
    "\n",
    "FALSO = Lojas com competidores à mais tempo vendem menos\n",
    "\n",
    "Em competition_time_month, o resultado é a Data da venda - Data da abertura do competidor.\n",
    "- Valores negativos -> Competidor abriu depois da venda\n",
    "- Valores positivos -> Competidor abriu antes da venda\n",
    "\n",
    "O Gráfico informa que quanto mais recente é a competição, maior é a venda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h3 = df4[['competition_time_month', 'sales']].groupby( 'competition_time_month' ).sum().reset_index()\n",
    "sns.barplot( h3, x='competition_time_month', y='sales');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h3 = df4[['competition_time_month', 'sales']].groupby( 'competition_time_month' ).sum().reset_index()\n",
    "h31 = h3[( h3['competition_time_month'] < 120 ) & (h3['competition_time_month'] != 0 )]\n",
    "sns.barplot( h31, x='competition_time_month', y='sales', );\n",
    "plt.xticks( rotation=90 );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grafico de regplot\n",
    "sns.regplot( h31, x ='competition_time_month', y='sales');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Força da correlação\n",
    "sns.heatmap( h3.corr( method='pearson' ), annot=True );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Lojas com promoções ativas por mais tempo deveriam vender mais.\n",
    "\n",
    "FALSO = Lojas com promoções ativas por mais tempo vendem menos\n",
    "\n",
    "- Valores negativo -> Venda realizada dentro do periodo tradicional de promoção\n",
    "- Valores positivo -> Venda realizada dentro do periodo de promoção extendida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h4 = df4[['promo_time_week', 'sales']].groupby( 'promo_time_week' ).sum().reset_index()\n",
    "sns.barplot( h4, x='promo_time_week', y='sales');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(2,1,1)\n",
    "extendido = h4[h4['promo_time_week']>0] # promoção extendida\n",
    "sns.barplot( extendido, x='promo_time_week', y='sales');\n",
    "# as vendas caem conforme o tempo de promoção\n",
    "\n",
    "plt.subplot(2,1,2)\n",
    "regular = h4[h4['promo_time_week']<0] # promoção regular\n",
    "sns.barplot( regular, x='promo_time_week', y='sales');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grafico de bolinhas\n",
    "plt.subplot(2,1,1)\n",
    "sns.regplot( extendido, x='promo_time_week', y='sales');\n",
    "plt.subplot(2,1,2)\n",
    "sns.regplot( regular, x='promo_time_week', y='sales');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Força da correlação\n",
    "sns.heatmap( h4.corr( method='pearson' ), annot=True );\n",
    "# para o heatmap precisa ser passado a feature inteira e não quebrada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<s>5. Lojas com mais dias de promoção deveriam vender mais.</s>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Lojas com mais promoções consecutivas deveriam vender mais.\n",
    "\n",
    "FALSA = Lojas com mais promoções consecutivas vendem menos\n",
    "\n",
    "- Verificado que quem participou da promoção extendida (promo2) vendeu menos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h5 = df4[['promo','promo2', 'sales']].groupby(['promo','promo2']).sum().sort_values('sales', ascending=False).reset_index()\n",
    "h5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verifiar comportamento durante o tempo\n",
    "\n",
    "aux1 = df4[( df4['promo'] == 1 ) & ( df4['promo2'] == 1 )][['year_week','sales']].groupby( 'year_week' ).sum().reset_index()\n",
    "ax = aux1.plot()\n",
    "\n",
    "aux2 = df4[( df4['promo'] == 1 ) & ( df4['promo2'] == 0 )][['year_week','sales']].groupby( 'year_week' ).sum().reset_index()\n",
    "\n",
    "aux2.plot( ax=ax )\n",
    "\n",
    "ax.legend( labels=['Tradicional & Extendida', 'Apenas Extendida']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Lojas abertas durante o feriado de Natal deveriam vender mais.\n",
    "\n",
    "FALSA = Lojas abertas durante o feriado de Natal não vendem mais do que os outros feriados\n",
    "\n",
    "- Dentro os feriadados,o feriado de Natal é o que menos vende, em nenhum ano."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a1 = df4[df4['state_holiday'] != 'regular_day']\n",
    "h7 = a1[['state_holiday', 'sales']].groupby( 'state_holiday' ).sum().reset_index()\n",
    "sns.barplot( h7, x='state_holiday', y='sales');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a2 = a1[['year', 'state_holiday', 'sales']].groupby( ['year','state_holiday']).sum().reset_index()\n",
    "sns.barplot( a2, x='year', y='sales', hue='state_holiday');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Lojas deveriam vender mais ao longo dos anos.\n",
    "\n",
    "FALSO = Lojas vendem menos ao logo dos anos.\n",
    "\n",
    "- As vendas vem caindo durante 2013 até 2015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h8 = df4[['year', 'sales']].groupby( ['year',]).sum().reset_index()\n",
    "sns.barplot( h8, x='year', y='sales');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.regplot( h8, x='year', y='sales');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap( h8.corr( method='pearson' ), annot=True );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Lojas deveriam vender mais no segundo semestre do ano.\n",
    "\n",
    "FALSO = As lojas vemde menos no segundo semestre.\n",
    "\n",
    "- A partir de Agosto as vendam caem drasticamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h9 = df4[['month', 'sales']].groupby( ['month']).sum().reset_index()\n",
    "sns.barplot( h9, x='month', y='sales');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.regplot( h9, x='month', y='sales');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap( h9.corr( method='pearson' ), annot=True );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10 Lojas deveriam vender mais depois do dia 10 de cada mês.\n",
    "\n",
    "*VERDADEIRA* - Bem óbvio porque depois do dia dia temos mais 20 dias para fechar o mês."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h10 = df4[['day', 'sales']].groupby( ['day']).sum().reset_index()\n",
    "sns.barplot( h10, x='day', y='sales');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.regplot( h10, x='day', y='sales');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap( h10.corr( method='pearson'), annot=True );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. Lojas deveriam vender menos aos finais de semana.\n",
    "\n",
    "*VERDADEIRA* - Situação parecida como acima, visto que fim de semana são apenas dois dias da semana."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h11 = df4[['day_of_week', 'sales']].groupby( ['day_of_week']).sum().reset_index()\n",
    "sns.barplot( h11, x='day_of_week', y='sales');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.regplot( h11, x='day_of_week', y='sales');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap( h11.corr( method='pearson' ), annot=True );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h10['antes_depois'] = h10['day'].apply(lambda x: 'antes_dia_10' if x <=10 else 'depois_dia_10')\n",
    "h111 = h10[['antes_depois','sales']].groupby( 'antes_depois').sum().reset_index()\n",
    "sns.barplot( h111, x='antes_depois', y='sales');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12. Lojas deveriam vender menos durante os feriados escolares.\n",
    "\n",
    "*VERDADEIRA* - Pois existem muito mais dias regulares do que dias com feriados escolares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h12 = df4[['school_holiday', 'sales']].groupby( ['school_holiday']).sum().reset_index()\n",
    "sns.barplot( h12, x='school_holiday', y='sales');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vendas por mês\n",
    "h121 =df4[['school_holiday', 'month', 'sales']].groupby( ['month','school_holiday']).sum().reset_index()\n",
    "sns.barplot( h121, x='month', y='sales', hue='school_holiday');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap( h12.corr( method='pearson' ), annot=True );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.1 Resumo das hipóteses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tab =[['Hipoteses', 'Conclusao', 'Relevancia'],\n",
    "        ['H1', 'Falsa', 'Baixa'],\n",
    "        ['H2', 'Falsa', 'Media'],\n",
    "        ['H3', 'Falsa', 'Media'],\n",
    "        ['H4', 'Falsa', 'Baixa'],\n",
    "        ['H5', '-', '-'],\n",
    "        ['H6', 'Falsa', 'Baixa'],\n",
    "        ['H7', 'Falsa', 'Media'],\n",
    "        ['H8', 'Falsa', 'Alta'],\n",
    "        ['H9', 'Falsa', 'Alta'],\n",
    "        ['H10', 'Verdadeira', 'Alta'],\n",
    "        ['H11', 'Verdadeira', 'Alta'],\n",
    "        ['H12', 'Verdadeira', 'Baixa'],\n",
    "]\n",
    "print( tabulate( tab, headers='firstrow' ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.1 Atributos Numéricos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlacao = numericos.corr(method='pearson')\n",
    "sns.heatmap(correlacao, annot=True)\n",
    "\n",
    "# quanto mais escuro, maior a correlação negativa.\n",
    "# quanto mais claro, maior a correlação positiva."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.2 Atributos Categóricos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = df4.select_dtypes(include='object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate cramer V\n",
    "a1 = cramer_v( a['state_holiday'], a['state_holiday'] )\n",
    "a2 = cramer_v( a['state_holiday'], a['store_type'] )\n",
    "a3 = cramer_v( a['state_holiday'], a['assortment'] )\n",
    "a4 = cramer_v( a['store_type'], a['state_holiday'] )\n",
    "a5 = cramer_v( a['store_type'], a['store_type'] )\n",
    "a6 = cramer_v( a['store_type'], a['assortment'] )\n",
    "a7 = cramer_v( a['assortment'], a['state_holiday'] )\n",
    "a8 = cramer_v( a['assortment'], a['store_type'] )\n",
    "a9 = cramer_v( a['assortment'], a['assortment'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final dataset\n",
    "d = pd.DataFrame( {'state_holiday': [a1, a2, a3], 'store_type': [a4, a5, a6], 'assortment': [a7, a8, a9] })\n",
    "d = d.set_index( d.columns )\n",
    "sns.heatmap( d, annot=True )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.0 PREPARAÇÃO DOS DADOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5 = df4.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Normalização"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalização =  (Variavel - Média) / desvio padrão\n",
    "\n",
    "Como encontrar as melhores variaveis ? \n",
    "\n",
    "- Nos resultados da 4.1 Analise univariada e 4.1.2 Variável numérica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Rescaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Min-Max Scaler = (Variavel - Média) / Valor Máximo - Valor Minimo\n",
    "\n",
    "Robust Scaler = (Variavel - Média) / Terceiro Quartil - Primeiro Quartil (quando temos outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = df5.select_dtypes(include=['int64','float64'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# competition_distance (Verificado no boxplot = possui muitos outliers)\n",
    "sns.boxplot(df5['competition_distance'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Como temos muitos outliers, usaremos o robust scaler\n",
    "rs = RobustScaler()\n",
    "df5['competition_distance'] = rs.fit_transform(df5[['competition_distance']].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# competition_time_month (Verificado no boxplot = possui muitos outliers)\n",
    "sns.boxplot(df5['competition_time_month'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Como temos muitos outliers, usaremos o robust scaler\n",
    "rs = RobustScaler()\n",
    "df5['competition_time_month'] = rs.fit_transform(df5[['competition_time_month']].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# promo_time_week (Verificado no boxplot = não possui muitos outliers)\n",
    "sns.boxplot(df5['promo_time_week'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Como não temos muitos outliers, usaremos o min-max scaler\n",
    "mms = MinMaxScaler()\n",
    "df5['promo_time_week'] = mms.fit_transform(df5[['promo_time_week']].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# year \n",
    "sns.boxplot(df5['year'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mms = MinMaxScaler()\n",
    "df5['year'] = mms.fit_transform(df5[['year']].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando arquivo PKL\n",
    "pickle.dump(rs, open(\"C:/Comunidade_DS/projetos/DS_Producao/parameters/competition_distance_scaler.pkl\",'wb'))\n",
    "pickle.dump(rs, open(\"C:/Comunidade_DS/projetos/DS_Producao/parameters/competition_time_month_scaler.pkl\",'wb'))\n",
    "pickle.dump(mms, open(\"C:/Comunidade_DS/projetos/DS_Producao/parameters/promo_time_week_scaler.pkl\",'wb'))\n",
    "pickle.dump(mms, open(\"C:/Comunidade_DS/projetos/DS_Producao/parameters/year_scaler.pkl\",'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Transformação"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.1 ENCODIG - Variavel CATEGÓRICA P/ variável NUMÉRICA\n",
    "\n",
    "One Hot Encoding - Para variáveis de estado\n",
    "\n",
    "Label Encoding - Para variáveis de nomes\n",
    "\n",
    "Ordinal Encoding - Segue uma ordenação numérica\n",
    "\n",
    "Target Encoding  -Para variáveis com muitos niveis\n",
    "\n",
    "Frequency Encoding\n",
    "\n",
    "Embedding Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categoricos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# state_holiday - Utilizando o one-hot encodingd\n",
    "df5 = pd.get_dummies(df5, prefix=['state_holiday'], columns=['state_holiday'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store_type - Utilizando o label encoding\n",
    "le = LabelEncoder()\n",
    "df5['store_type'] = le.fit_transform(df5['store_type'].ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(le, open(\"C:/Comunidade_DS/projetos/DS_Producao/parameters/store_type_scaler.pkl\",'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assortments - Utilizando o ordinal encoding\n",
    "dicionario = {'basic' : 1, 'extra': 2, 'extended' : 3}\n",
    "df5['assortment'] = df5['assortment'].map(dicionario)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.2 Transformação de Grandeza"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trazer a variavel Resposta \"Sales\" para o mais proximo possivel de uma distribuição NORMAL\n",
    "\n",
    "- Transformação Logaritmica\n",
    "- Box-Cox Transformation\n",
    "- Cube-Root Transformation\n",
    "- Square-Root Transformation\n",
    "- Sine and Cosine Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot( df5['sales'], kde=False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variavel Resposta é ['sales']\n",
    "df5['sales']=np.log1p (df5['sales'])\n",
    "sns.histplot(df5['sales'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.3 Transformação de Natureza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# day_of_week\n",
    "df5['day_of_week_sin'] = df5['day_of_week'].apply( lambda x: np.sin( x * ( 2. *np.pi/7 ) ) )\n",
    "df5['day_of_week_cos'] = df5['day_of_week'].apply( lambda x: np.cos( x * ( 2. *np.pi/7 ) ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# month\n",
    "df5['month_sin'] = df5['month'].apply( lambda x: np.sin( x * ( 2. *np.pi/12 ) ) )\n",
    "df5['month_cos'] = df5['month'].apply( lambda x: np.cos( x * ( 2. *np.pi/12 ) ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# day\n",
    "df5['day_sin'] = df5['day'].apply( lambda x: np.sin( x * ( 2. *np.pi/30 ) ) )\n",
    "df5['day_cos'] = df5['day'].apply( lambda x: np.cos( x * ( 2. *np.pi/30 ) ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# week of year\n",
    "df5['week_of_year_sin'] = df5['week_of_year'].apply( lambda x: np.sin( x * ( 2. *np.pi/52 ) ) )\n",
    "df5['week_of_year_cos'] = df5['week_of_year'].apply( lambda x: np.cos( x * ( 2. *np.pi/52 ) ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.0 SELEÇÃO DE VARIÁVEIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df6=df5.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Dataframe de Treino e Teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropadas = ['week_of_year', 'day', 'month', 'day_of_week', 'promo_since','competition_since', 'year_week' ]\n",
    "df6 = df6.drop( dropadas, axis=1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dados de treino\n",
    "x_treino = df6[df6['date'] < '2015-06-19']\n",
    "y_treino = x_treino['sales']\n",
    "\n",
    "# Dados de teste\n",
    "x_teste = df6[df6['date'] >= '2015-06-19']\n",
    "y_teste = x_teste['sales']\n",
    "\n",
    "print( 'Data mínima do treino : {}'.format( x_treino['date'].min() ) )\n",
    "print( 'Data máxima do treino : {}'.format( x_treino['date'].max() ) )\n",
    "\n",
    "print( '\\nData mínima do teste : {}'.format( x_teste['date'].min() ) )\n",
    "print( 'Data máxima do teste : {}'.format( x_teste['date'].max() ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Boruta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dataset de treino e teste do Boruta\n",
    "\n",
    "#x_treino_n = x_treino.drop( ['date', 'sales'], axis=1 ).values\n",
    "#y_treino_n = y_treino.values.ravel()\n",
    "\n",
    "## RandomForestRegressor\n",
    "#rf = RandomForestRegressor( n_jobs=-1 )\n",
    "\n",
    "## Boruta\n",
    "#boruta = BorutaPy( rf, n_estimators='auto', verbose=2, random_state=42 ).fit(x_treino_n, y_treino_n )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.1 Colunas Escolhidas pelo Boruta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cols_selected = boruta.support_.tolist()\n",
    "\n",
    "## melhores colunas\n",
    "#x_treino_fs = x_treino.drop( ['date', 'sales'], axis=1 )\n",
    "#cols_boruta = x_treino_fs.iloc[:, cols_selected].columns.to_list()\n",
    "\n",
    "## not selected boruta\n",
    "#cols_not_selected_boruta = list( np.setdiff1d( x_treino_fs.columns,֒cols_selected_boruta ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model definition\n",
    "forest = RandomForestRegressor( n_estimators=100, random_state=0, n_jobs=-1 )\n",
    "\n",
    "# data preparation\n",
    "x_treino_n = x_treino.drop( ['date', 'sales'], axis=1 )\n",
    "y_treino_n = y_treino.values\n",
    "forest.fit( x_treino_n, y_treino_n )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = forest.feature_importances_\n",
    "std = np.std([tree.feature_importances_ for tree in forest.estimators_], axis=0)\n",
    "indices = np.argsort(importances)[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print the feature ranking\n",
    "print('Ranking das Variáveis')\n",
    "df = pd.DataFrame()\n",
    "for i, j in zip (x_treino, forest.feature_importances_):\n",
    "    aux = pd.DataFrame({'Variavel:' : i, 'Importancia:': j}, index=[0])\n",
    "    df = pd.concat([df,aux], axis=0)\n",
    "\n",
    "\n",
    "print(df.sort_values('Importancia:', ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 Seleção Manual de Colunas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_boruta = [\n",
    "'store',\n",
    "'promo',\n",
    "'store_type',\n",
    "'assortment',\n",
    "'competition_distance',\n",
    "'competition_open_since_month',\n",
    "'competition_open_since_year',\n",
    "'promo2',\n",
    "'promo2_since_week',\n",
    "'promo2_since_year',\n",
    "'competition_time_month',\n",
    "'promo_time_week',\n",
    "'day_of_week_sin',\n",
    "'day_of_week_cos',\n",
    "'month_sin',\n",
    "'month_cos',\n",
    "'day_sin',\n",
    "'day_cos',\n",
    "'week_of_year_sin',\n",
    "'week_of_year_cos']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inserir outras colunas\n",
    "feat_to_add = ['date', 'sales']\n",
    "\n",
    "# Final Features\n",
    "cols_boruta_full = cols_boruta.copy()\n",
    "cols_boruta_full.extend( feat_to_add )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.0 TREINAMENTO DE MACHINE LEARNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_treino[ cols_boruta ]\n",
    "x_test = x_teste[cols_boruta ]\n",
    "\n",
    "# Time Series Data Preparation\n",
    "x_training = x_treino[ cols_boruta_full ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 Modelo de Média"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux1 = x_test.copy()\n",
    "aux1['sales'] = y_teste.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux2 = aux1[['store','sales']].groupby('store').mean().reset_index().rename(columns={'sales': 'Predicao'} )\n",
    "aux1 = pd.merge( aux1, aux2, how='left', on='store' )\n",
    "y_estimado_baseline = aux1['Predicao']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#performance\n",
    "baseline_result = ml_error( 'Modelo de Média', np.expm1( y_teste ), np.expm1(y_estimado_baseline ) )\n",
    "baseline_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 Modelo de Regressão Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modelo\n",
    "lr = LinearRegression().fit(x_train,y_treino)\n",
    "\n",
    "# prevdição\n",
    "y_pred_lr = lr.predict(x_test)\n",
    "\n",
    "# performance\n",
    "lr_result = ml_error( 'Regressão Linear', np.expm1( y_teste ), np.expm1(y_pred_lr) )\n",
    "lr_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2.1 Modelo de Regressão Linear - Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_result_cv = cross_validation( x_training, 5, 'Regressão Linear', lr)\n",
    "lr_result_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3 LASSO - Modelo de Regressão Linear Regular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modelo\n",
    "lrr = Lasso(alpha=0.01).fit(x_train,y_treino)\n",
    "\n",
    "# prevdição\n",
    "y_pred_lrr = lrr.predict(x_test)\n",
    "\n",
    "# performance\n",
    "lrr\n",
    "lrr_result = ml_error( 'Lasso', np.expm1( y_teste ), np.expm1(y_pred_lrr) )\n",
    "lrr_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3.1 LASSO - Modelo de Regressão Linear Regular - Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrr_result_cv = cross_validation( x_training, 5, 'Lasso', lrr)\n",
    "lrr_result_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.4 Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "rf = RandomForestRegressor( n_estimators=100, n_jobs=-1, random_state=42 ).fit(x_train,y_treino)\n",
    "\n",
    "# prediction\n",
    "y_pred_rf = rf.predict( x_test )\n",
    "# performance\n",
    "\n",
    "rf_result = ml_error( 'Random Forest Regressor', np.expm1( y_teste ), np.expm1(y_pred_rf ) )\n",
    "rf_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4.1 Random Forest Regressor - Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_result_cv = cross_validation( x_training, 5, 'Random Forest Regressor', rf)\n",
    "rf_result_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.5 XGBoost Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modelo\n",
    "modelo_xgb = xgb.XGBRegressor( objective='reg:squarederror', \n",
    "                                n_estimators=100, \n",
    "                                #learning_rate=0.01, \n",
    "                                max_depth=10, \n",
    "                                subsample=0.7,\n",
    "                                colsample_bytree=0.9  ).fit(x_train,y_treino)\n",
    "                                \n",
    "#modelo_xgb = xgb.XGBRegressor(objective='reg:squarederror').fit(x_train,y_treino)\n",
    "\n",
    "# prevdição\n",
    "y_pred_xgb = modelo_xgb.predict(x_test)\n",
    "\n",
    "# performance\n",
    "xgb_result=ml_error( 'XGBoost Regressor', np.expm1( y_teste ), np.expm1(y_pred_xgb) )\n",
    "xgb_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.5.1 XGBoost Regressor - Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_result_cv = cross_validation( x_training, 5, 'XGBoost Regressor', modelo_xgb)\n",
    "xgb_result_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.6 Comparação do resultado de todos os modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelling_result = pd.concat( [baseline_result, lr_result, lrr_result,rf_result, xgb_result] )\n",
    "modelling_result.sort_values( 'RMSE' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.6.2 Real Performance - Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelling_result_cv = pd.concat( [lr_result_cv, lrr_result_cv,rf_result_cv, xgb_result_cv] )\n",
    "modelling_result_cv.sort_values( 'RMSE CV' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8.0 FINE TUNING - Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parametros = {'n_estimators': [100],\n",
    "              'max_depth': [3,5,9],\n",
    "              'subsample': [0.1, 0.5,0.75],\n",
    "              'colsample_bytree': [0.3, 0.5, 0.9],\n",
    "              'min_child_weight' : [3,8,15]}\n",
    "\n",
    "MAX_EVAL = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultado_final = pd.DataFrame()\n",
    "\n",
    "for i in range(MAX_EVAL):\n",
    "    lista = { k: random.sample( v, 1 )[0] for k, v in parametros.items() }\n",
    "    print( lista )\n",
    "\n",
    "    modelo_xgb = xgb.XGBRegressor( objective='reg:squarederror', \n",
    "                                n_estimators=lista['n_estimators'], \n",
    "                                max_depth=lista['max_depth'], \n",
    "                                subsample=lista['subsample'],\n",
    "                                colsample_bytree=lista['colsample_bytree'],\n",
    "                                min_child_weight=lista['min_child_weight'])\n",
    "\n",
    "\n",
    "    resultado = cross_validation( x_training, 5, 'XGBoost Regressor', modelo_xgb)\n",
    "    resultado_final = pd.concat( [resultado_final, resultado] )\n",
    "\n",
    "resultado_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parametros_final = {'n_estimators': 100,\n",
    "              'max_depth': 9,\n",
    "              'subsample': 0.75,\n",
    "              'colsample_bytree': 0.5,\n",
    "              'min_child_weight' : 8}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modelo \n",
    "modelo_xgb_tunado = xgb.XGBRegressor( objective='reg:squarederror', \n",
    "                                n_estimators=parametros_final['n_estimators'], \n",
    "                                max_depth=parametros_final['max_depth'], \n",
    "                                subsample=parametros_final['subsample'],\n",
    "                                colsample_bytree=parametros_final['colsample_bytree'],\n",
    "                                min_child_weight=parametros_final['min_child_weight']).fit(x_train,y_treino)\n",
    "\n",
    "# predição\n",
    "y_pred_xgb_tunado = modelo_xgb_tunado.predict(x_test)\n",
    "\n",
    "# performance\n",
    "xgb_result_tunado = ml_error( 'XGBoost Regressor', np.expm1( y_teste ), np.expm1(y_pred_xgb_tunado) )\n",
    "xgb_result_tunado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9.0 TRADUÇÃO E INTERPRETAÇÃO DO ERRO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df9 = x_teste[cols_boruta_full]\n",
    "\n",
    "# Ajustar para escala original (rescale)\n",
    "df9['sales'] = np.expm1(df9['sales'])\n",
    "df9['predictions'] = np.expm1(y_pred_xgb_tunado)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.1 Performance de Negócio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Soma das predições\n",
    "soma = df9[['store','predictions']].groupby('store').sum().reset_index()\n",
    "\n",
    "# MAE e MAPE\n",
    "df9_mae = df9[['store','sales','predictions']].groupby('store').apply(lambda x: mean_absolute_error (x['sales'],x['predictions'] )).reset_index().rename(columns={0:'MAE'})\n",
    "df9_mape = df9[['store','sales','predictions']].groupby('store').apply(lambda x: mean_absolute_percentage_error (x['sales'],x['predictions'] )).reset_index().rename(columns={0:'MAPE'})\n",
    "\n",
    "# Merge\n",
    "df9_merge = pd.merge(df9_mae,df9_mape,how='inner', on='store')\n",
    "df_mergeado = pd.merge(soma,df9_merge,how='inner', on='store')\n",
    "\n",
    "# Cenários\n",
    "df_mergeado['pior_cenario'] = df_mergeado['predictions'] - df_mergeado['MAE']\n",
    "df_mergeado['melhor_cenario'] = df_mergeado['predictions'] + df_mergeado['MAE']\n",
    "\n",
    "#Ordenação Colunas\n",
    "df_mergeado = df_mergeado[['store','predictions', 'pior_cenario', 'melhor_cenario', 'MAE', 'MAPE']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mergeado.sort_values('MAPE', ascending=True).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot( x='store', y='MAPE', data=df_mergeado )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.2 Performance Total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = df_mergeado[['predictions', 'pior_cenario', 'melhor_cenario']].apply( lambda x:np.sum( x ), axis=0 ).reset_index().rename( columns={'index': 'Scenario', 0:'Values'} )\n",
    "total['Values'] = total['Values'].map( 'R${:,.2f}'.format )\n",
    "total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.3 Performance Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df9['error'] = df9['sales'] - df9['predictions']\n",
    "df9['error_rate'] = df9['predictions'] / df9['sales']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot( 2, 2, 1 )\n",
    "sns.lineplot( x='date', y='sales', data=df9, label='SALES' )\n",
    "sns.lineplot( x='date', y='predictions', data=df9, label='PREDICTIONS' )\n",
    "\n",
    "plt.subplot( 2, 2, 2 )\n",
    "sns.lineplot( x='date', y='error_rate', data=df9 )\n",
    "plt.axhline( 1, linestyle='--')\n",
    "\n",
    "plt.subplot( 2, 2, 3 )\n",
    "sns.distplot( df9['error'] )\n",
    "\n",
    "plt.subplot( 2, 2, 4 )\n",
    "sns.scatterplot(x=df9['predictions'],y=df9['error'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10.0. DEPLOY PARA PRODUÇÃO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modelo treinado\n",
    "pickle.dump(modelo_xgb_tunado, open(\"C:/Comunidade_DS/projetos/DS_Producao/model/modelo_rossmann.pkl\",'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.1 Rossmann Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import inflection\n",
    "import numpy as pd\n",
    "import math\n",
    "import datetime\n",
    "\n",
    "class Rossmann (object):\n",
    "    def __init__ (self):\n",
    "        self.home.path = \"C:/Comunidade_DS/projetos/DS_Producao/\"\n",
    "        self.competition_distance_scaler = pickle.load(open(self.home_path + 'parameters/competition_distance_scaler.pkl', 'rb'))\n",
    "        self.competition_time_month_scaler = pickle.load(open(self.home_path + 'parameters/competition_time_month_scaler.pkl', 'rb'))\n",
    "        self.promo_time_week_scaler = pickle.load(open(self.home_path + 'parameters/promo_time_week_scaler.pkl', 'rb'))\n",
    "        self.year_scaler = pickle.load(open(self.home_path + 'parameters/year_scaler.pkl', 'rb'))\n",
    "        self.store_type_scaler = pickle.load(open(self.home_path + 'parameters/store_type_scaler.pkl', 'rb'))\n",
    "\n",
    "    def limpeza_dados(self,df1):\n",
    "        colunas_original = ['Store', 'DayOfWeek', 'Date', 'Open', 'Promo',\n",
    "                            'StateHoliday', 'SchoolHoliday', 'StoreType', 'Assortment',\n",
    "                            'CompetitionDistance', 'CompetitionOpenSinceMonth',\n",
    "                            'CompetitionOpenSinceYear', 'Promo2', 'Promo2SinceWeek',\n",
    "                            'Promo2SinceYear', 'PromoInterval']\n",
    "                            \n",
    "        snakecase = lambda x: inflection.underscore(x) \n",
    "        colunas_novas = list(map(snakecase, colunas_original))\n",
    "        df1.columns = colunas_novas\n",
    "\n",
    "        df1['date'] = pd.to_datetime( df1['date'] )\n",
    "\n",
    "        df1['competition_distance'] = df1['competition_distance'].apply(lambda x: 200000 if math.isnan(x) else x )       \n",
    "        df1['competition_open_since_month'] = df1.apply(lambda x: x['date'].month if math.isnan(x['competition_open_since_month']) else x['competition_open_since_month'],axis=1)\n",
    "        df1['competition_open_since_year'] = df1.apply(lambda x: x['date'].year if math.isnan(x['competition_open_since_year']) else x['competition_open_since_year'],axis=1)\n",
    "        df1['promo2_since_week'] = df1.apply(lambda x: x['date'].week if math.isnan(x['promo2_since_week']) else x['promo2_since_week'],axis=1)\n",
    "        df1['promo2_since_year'] = df1.apply(lambda x: x['date'].year if math.isnan(x['promo2_since_year']) else x['promo2_since_year'],axis=1)\n",
    "        month_map = {1: 'Jan',2: 'Fev',3: 'Mar',4: 'Apr',5: 'May',6: 'Jun',7: 'Jul',8: 'Aug', 9: 'Sep',10: 'Oct',11: 'Nov',12: 'Dec'}\n",
    "        df1['promo_interval'].fillna(0, inplace=True)\n",
    "        df1['month_map'] = df1['date'].dt.month.map(month_map)\n",
    "        df1['is_promo'] = df1[['promo_interval','month_map']].apply( lambda x: 0 if x['promo_interval'] == 0 else 1 if x['month_map'] in x['promo_interval'].split(',') else 0, axis=1)\n",
    "\n",
    "        df1['competition_open_since_month'] = df1['competition_open_since_month'].astype('int64')\n",
    "        df1['competition_open_since_year'] = df1['competition_open_since_year'].astype('int64')\n",
    "        df1['promo2_since_week'] = df1['promo2_since_week'].astype('int64')\n",
    "        df1['promo2_since_year'] = df1['promo2_since_year'].astype('int64')\n",
    "\n",
    "        return df1\n",
    "\n",
    "\n",
    "    def feature_engineering(self,df2):\n",
    "\n",
    "        df2['year'] = df2['date'].dt.year\n",
    "        df2['month'] = df2['date'].dt.month\n",
    "        df2['day'] = df2['date'].dt.day\n",
    "        df2['week_of_year'] = df2['date'].dt.isocalendar().week\n",
    "        df2['year_week'] = df2['date'].dt.strftime('%Y-%W')\n",
    "        df2['competition_since'] = df2.apply(lambda x: datetime.datetime(year=x['competition_open_since_year'], month=x['competition_open_since_month'], day= 1), axis=1)\n",
    "        df2['competition_time_month'] = ((df2['date'] - df2['competition_since'])/30).apply(lambda x: x.days).astype(int)\n",
    "        df2['promo_since'] = df2['promo2_since_year'].astype(str) + '-' +df2['promo2_since_week'].astype(str)\n",
    "        df2['promo_since'] = df2['promo_since'].apply( lambda x: datetime.datetime.strptime( x + '-1', '%Y-%W-%w' ) - datetime.timedelta( days=7 ) )\n",
    "        df2['promo_time_week'] = ( ( df2['date'] - df2['promo_since'] )/7 ).apply(lambda x: x.days ).astype( int )\n",
    "        df2['assortment'] = df2['assortment'].apply( lambda x: 'basic' if x == 'a' else 'extra' if x == 'b' else 'extended' )\n",
    "        df2['state_holiday'] = df2['state_holiday'].apply( lambda x: 'public_holiday' if x == 'a' else 'easter_holiday' if x == 'b' else 'christmas' if x == 'c' else 'regular_day' )\n",
    "\n",
    "        df2 = df2[(df2['open'] != 0)] \n",
    "        colunas_para_dropar = ['open', 'promo_interval','month_map']\n",
    "        df2 = df2.drop(colunas_para_dropar, axis=1)\n",
    "\n",
    "        return df2\n",
    "    \n",
    "    def preparacao (self,df5):\n",
    "        \n",
    "        df5['competition_distance'] = self.competition_distance.scaler.fit_transform(df5[['competition_distance']].values)\n",
    "        df5['competition_time_month']= self.competition_time_month_scaler.fit_transform(df5[['competition_time_month']].values)\n",
    "        df5['promo_time_week']= self.promo_time_week_scaler.fit_transform(df5[['promo_time_week']].values)\n",
    "        df5['year']= self.year_scaler.fit_transform(df5[['year']].values)\n",
    "\n",
    "        df5 = pd.get_dummies(df5, prefix=['state_holiday'], columns=['state_holiday'])\n",
    "        df5['store_type'] = self.store_type_scaler.fit_transform(df5['store_type'])\n",
    "\n",
    "        dicionario = {'basic':1, 'extra':2,'extended':3 }\n",
    "        df5['assortment'] = df5['assortment'].map(dicionario)\n",
    "\n",
    "        df5['day_of_week_sin'] = df5['day_of_week'].apply( lambda x: np.sin( x * ( 2. *np.pi/7 ) ) )\n",
    "        df5['day_of_week_cos'] = df5['day_of_week'].apply( lambda x: np.cos( x * ( 2. *np.pi/7 ) ) )\n",
    "\n",
    "        df5['month_sin'] = df5['month'].apply( lambda x: np.sin( x * ( 2. *np.pi/12 ) ) )\n",
    "        df5['month_cos'] = df5['month'].apply( lambda x: np.cos( x * ( 2. *np.pi/12 ) ) )\n",
    "\n",
    "        df5['day_sin'] = df5['day'].apply( lambda x: np.sin( x * ( 2. * np.pi/30 ) ) )\n",
    "        df5['day_cos'] = df5['day'].apply( lambda x: np.cos( x * ( 2. * np.pi/30 ) ) )\n",
    "\n",
    "        # week of year\n",
    "        df5['week_of_year_sin'] = df5['week_of_year'].apply( lambda x: np.sin( x * ( 2. * np.pi/52 ) ) )\n",
    "        df5['week_of_year_cos'] = df5['week_of_year'].apply( lambda x: np.cos( x * ( 2. * np.pi/52 ) ) )\n",
    "\n",
    "        colunas_escolhidas = ['store',\n",
    "                            'promo',\n",
    "                            'store_type',\n",
    "                            'assortment',\n",
    "                            'competition_distance',\n",
    "                            'competition_open_since_month',\n",
    "                            'competition_open_since_year',\n",
    "                            'promo2',\n",
    "                            'promo2_since_week',\n",
    "                            'promo2_since_year',\n",
    "                            'competition_time_month',\n",
    "                            'promo_time_week',\n",
    "                            'day_of_week_sin',\n",
    "                            'day_of_week_cos',\n",
    "                            'month_sin',\n",
    "                            'month_cos',\n",
    "                            'day_sin',\n",
    "                            'day_cos',\n",
    "                            'week_of_year_sin',\n",
    "                            'week_of_year_cos']\n",
    "\n",
    "        return df5[colunas_escolhidas] \n",
    "    \n",
    "    def get_prediction( self, model, original_data, test_data ):\n",
    "        # prediction\n",
    "        pred = model.predict( test_data )\n",
    "        \n",
    "        # join pred into the original data\n",
    "        original_data['prediction'] = np.expm1( pred )\n",
    "        \n",
    "        return original_data.to_json( orient='records', date_format='iso' ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.2 API Handler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "from flask import Flask, request, Response\n",
    "from rossmann.rossmann import Rossmann"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# carregar o modelo atual\n",
    "modelo = pickle.load( open(\"C:/Comunidade_DS/projetos/DS_Producao/model/modelo_rossmann.pkl\", 'rb') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize API\n",
    "app = Flask( __name__ )\n",
    "@app.route( '/rossmann/predict', methods=['POST'] )\n",
    "\n",
    "def rossmann_predict():\n",
    "    test_json = request.get_json()\n",
    "    \n",
    "    if test_json: # there is data\n",
    "        if isinstance( test_json, dict ): # unique example\n",
    "            test_raw = pd.DataFrame( test_json, index=[0] )\n",
    "\n",
    "        else: # multiple example\n",
    "            test_raw = pd.DataFrame( test_json, columns=test_json[0].keys() )\n",
    "\n",
    "        # Instantiate Rossmann class\n",
    "        pipeline = Rossmann()\n",
    "\n",
    "        # data cleaning\n",
    "        df1 = pipeline.data_cleaning( test_raw )\n",
    "\n",
    "        # feature engineering\n",
    "        df2 = pipeline.feature_engineering( df1 )\n",
    "\n",
    "        # data preparation\n",
    "        df3 = pipeline.data_preparation( df2 )\n",
    "\n",
    "        # prediction\n",
    "        df_response = pipeline.get_prediction( modelo, test_raw, df3 )\n",
    "\n",
    "        return df_response\n",
    "    \n",
    "    else:\n",
    "        return Response( '{}', status=200, mimetype='application/json' )\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run( '0.0.0.0' , debug=True )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.3 API de Teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# carregar o dataset de teste\n",
    "df10 = pd.read_csv( \"C:/Comunidade_DS/projetos/DS_Producao/datasets/test.csv\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge test dataset + store\n",
    "df_test = pd.merge( df10, df_store_raw, how='left', on='Store' )\n",
    "\n",
    "# choose store for prediction\n",
    "df_test = df_test[df_test['Store'].isin( [1,2,3] )]\n",
    "\n",
    "# remove closed days\n",
    "df_test = df_test[df_test['Open'] != 0]\n",
    "df_test = df_test[~df_test['Open'].isnull()] #lojas diferentes de zero\n",
    "df_test = df_test.drop( 'Id', axis=1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert Dataframe to json\n",
    "data = json.dumps( df_test.to_dict( orient='records' ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status Code 200\n"
     ]
    }
   ],
   "source": [
    "# API Call\n",
    "\n",
    "# url = 'http://localhost:5000/rossmann/predict'\n",
    "# url = 'https://rossmann-api-h4ab.onrender.com/rossmann/predict'\n",
    "url = 'https://arss-rossmann-api.onrender.com/rossmann/predict'\n",
    "header = {'Content-type': 'application/json'}\n",
    "data = data\n",
    "\n",
    "r = requests.post(url, data = data, headers = header)\n",
    "print('Status Code {}' .format(r.status_code))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = pd.DataFrame( r.json(), columns=r.json()[0].keys()  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A loja 1 deverá vender R$334,251.89 nas próximas 6 semanas\n",
      "A loja 3 deverá vender R$249,256.34 nas próximas 6 semanas\n"
     ]
    }
   ],
   "source": [
    "d2 = d1[['store', 'prediction']].groupby( 'store' ).sum().reset_index()\n",
    "\n",
    "for i in range( len( d2 ) ):\n",
    "    print( 'A loja {} deverá vender R${:,.2f} nas próximas 6 semanas'.format(d2.loc[i, 'store'],d2.loc[i, 'prediction'] ) )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
